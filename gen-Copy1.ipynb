{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f1fb7b-02ae-420b-8d7d-f0b5dd3299bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Load and Preprocess Data\n",
    "filename = \"mental_H.txt\"\n",
    "with open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "    raw_text = file.read().lower()\n",
    "import re\n",
    "\n",
    "# Remove unnecessary characters and normalize text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = text.strip().lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "raw_text = clean_text(raw_text)\n",
    "\n",
    "# Tokenization\n",
    "##\n",
    "tokenizer = Tokenizer(num_words=5000)  # Limit vocab size to 5000\n",
    "tokenizer.fit_on_texts([raw_text])\n",
    "##\n",
    "sequences = tokenizer.texts_to_sequences([raw_text])[0]\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "##\n",
    "seq_length = 150\n",
    "\n",
    "# Create Input-Output Pairs\n",
    "X = []\n",
    "y = []\n",
    "for i in range(0, len(sequences) - seq_length):\n",
    "    X.append(sequences[i:i + seq_length])\n",
    "    y.append(sequences[i + seq_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "# Reshape for LSTM Input\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1]))\n",
    "\n",
    "# Model Definition\n",
    "##\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 256, input_length=seq_length),\n",
    "    Bidirectional(LSTM(256, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    Bidirectional(LSTM(256)),\n",
    "    Dropout(0.3),\n",
    "    Dense(vocab_size, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a27254-b1a5-4f8d-9028-b566fafcc222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 150, 256)          1294592   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 150, 512)         1050624   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 150, 512)          0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 150, 512)         2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 512)              1574912   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5057)              2594241   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,516,417\n",
      "Trainable params: 6,515,393\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e75f473-e0a1-4d62-aae4-b8f3779a7a17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 6.6721 - accuracy: 0.0630\n",
      "Epoch 1: loss improved from inf to 6.67208, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 58s 79ms/step - loss: 6.6721 - accuracy: 0.0630\n",
      "Epoch 2/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 6.2044 - accuracy: 0.1013\n",
      "Epoch 2: loss improved from 6.67208 to 6.20436, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 54s 80ms/step - loss: 6.2044 - accuracy: 0.1013\n",
      "Epoch 3/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 5.9117 - accuracy: 0.1215\n",
      "Epoch 3: loss improved from 6.20436 to 5.91173, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 53s 79ms/step - loss: 5.9117 - accuracy: 0.1215\n",
      "Epoch 4/100\n",
      "674/675 [============================>.] - ETA: 0s - loss: 5.6580 - accuracy: 0.1382\n",
      "Epoch 4: loss improved from 5.91173 to 5.65807, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 5.6581 - accuracy: 0.1382\n",
      "Epoch 5/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 5.3866 - accuracy: 0.1549\n",
      "Epoch 5: loss improved from 5.65807 to 5.38665, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 5.3866 - accuracy: 0.1549\n",
      "Epoch 6/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 5.1051 - accuracy: 0.1698\n",
      "Epoch 6: loss improved from 5.38665 to 5.10505, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 53s 78ms/step - loss: 5.1051 - accuracy: 0.1698\n",
      "Epoch 7/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 4.8077 - accuracy: 0.1868\n",
      "Epoch 7: loss improved from 5.10505 to 4.80767, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 53s 78ms/step - loss: 4.8077 - accuracy: 0.1868\n",
      "Epoch 8/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 4.4980 - accuracy: 0.2070\n",
      "Epoch 8: loss improved from 4.80767 to 4.49799, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 54s 80ms/step - loss: 4.4980 - accuracy: 0.2070\n",
      "Epoch 9/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 4.2379 - accuracy: 0.2250\n",
      "Epoch 9: loss improved from 4.49799 to 4.23794, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 54s 79ms/step - loss: 4.2379 - accuracy: 0.2250\n",
      "Epoch 10/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 3.9037 - accuracy: 0.2542\n",
      "Epoch 10: loss improved from 4.23794 to 3.90370, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 3.9037 - accuracy: 0.2542\n",
      "Epoch 11/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 3.5960 - accuracy: 0.2883\n",
      "Epoch 11: loss improved from 3.90370 to 3.59599, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 54s 80ms/step - loss: 3.5960 - accuracy: 0.2883\n",
      "Epoch 12/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 3.3130 - accuracy: 0.3230\n",
      "Epoch 12: loss improved from 3.59599 to 3.31295, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 54s 80ms/step - loss: 3.3130 - accuracy: 0.3230\n",
      "Epoch 13/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 3.0460 - accuracy: 0.3630\n",
      "Epoch 13: loss improved from 3.31295 to 3.04601, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 82ms/step - loss: 3.0460 - accuracy: 0.3630\n",
      "Epoch 14/100\n",
      "674/675 [============================>.] - ETA: 0s - loss: 2.8188 - accuracy: 0.3983\n",
      "Epoch 14: loss improved from 3.04601 to 2.81902, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 54s 80ms/step - loss: 2.8190 - accuracy: 0.3982\n",
      "Epoch 15/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 2.6033 - accuracy: 0.4333\n",
      "Epoch 15: loss improved from 2.81902 to 2.60326, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 53s 78ms/step - loss: 2.6033 - accuracy: 0.4333\n",
      "Epoch 16/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 2.4215 - accuracy: 0.4637\n",
      "Epoch 16: loss improved from 2.60326 to 2.42151, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 54s 79ms/step - loss: 2.4215 - accuracy: 0.4637\n",
      "Epoch 17/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 2.2628 - accuracy: 0.4889\n",
      "Epoch 17: loss improved from 2.42151 to 2.26282, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 54s 80ms/step - loss: 2.2628 - accuracy: 0.4889\n",
      "Epoch 18/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 2.1119 - accuracy: 0.5198\n",
      "Epoch 18: loss improved from 2.26282 to 2.11189, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 85ms/step - loss: 2.1119 - accuracy: 0.5198\n",
      "Epoch 19/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 1.9743 - accuracy: 0.5417\n",
      "Epoch 19: loss improved from 2.11189 to 1.97434, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 1.9743 - accuracy: 0.5417\n",
      "Epoch 20/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 1.8510 - accuracy: 0.5645\n",
      "Epoch 20: loss improved from 1.97434 to 1.85102, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 1.8510 - accuracy: 0.5645\n",
      "Epoch 21/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 1.7238 - accuracy: 0.5912\n",
      "Epoch 21: loss improved from 1.85102 to 1.72379, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 85ms/step - loss: 1.7238 - accuracy: 0.5912\n",
      "Epoch 22/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 1.6212 - accuracy: 0.6136\n",
      "Epoch 22: loss improved from 1.72379 to 1.62119, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 58s 85ms/step - loss: 1.6212 - accuracy: 0.6136\n",
      "Epoch 23/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 1.5234 - accuracy: 0.6337\n",
      "Epoch 23: loss improved from 1.62119 to 1.52342, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 58s 86ms/step - loss: 1.5234 - accuracy: 0.6337\n",
      "Epoch 24/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 1.4207 - accuracy: 0.6533\n",
      "Epoch 24: loss improved from 1.52342 to 1.42074, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 1.4207 - accuracy: 0.6533\n",
      "Epoch 25/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 1.3335 - accuracy: 0.6714\n",
      "Epoch 25: loss improved from 1.42074 to 1.33353, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 84ms/step - loss: 1.3335 - accuracy: 0.6714\n",
      "Epoch 26/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 1.2591 - accuracy: 0.6855\n",
      "Epoch 26: loss improved from 1.33353 to 1.25909, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 1.2591 - accuracy: 0.6855\n",
      "Epoch 27/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 1.1732 - accuracy: 0.7061\n",
      "Epoch 27: loss improved from 1.25909 to 1.17321, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 1.1732 - accuracy: 0.7061\n",
      "Epoch 28/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 1.1058 - accuracy: 0.7217\n",
      "Epoch 28: loss improved from 1.17321 to 1.10576, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 58s 85ms/step - loss: 1.1058 - accuracy: 0.7217\n",
      "Epoch 29/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 1.0417 - accuracy: 0.7388\n",
      "Epoch 29: loss improved from 1.10576 to 1.04169, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 1.0417 - accuracy: 0.7388\n",
      "Epoch 30/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.9802 - accuracy: 0.7487\n",
      "Epoch 30: loss improved from 1.04169 to 0.98020, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 84ms/step - loss: 0.9802 - accuracy: 0.7487\n",
      "Epoch 31/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.9212 - accuracy: 0.7630\n",
      "Epoch 31: loss improved from 0.98020 to 0.92120, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 0.9212 - accuracy: 0.7630\n",
      "Epoch 32/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.8707 - accuracy: 0.7743\n",
      "Epoch 32: loss improved from 0.92120 to 0.87070, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 85ms/step - loss: 0.8707 - accuracy: 0.7743\n",
      "Epoch 33/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.8211 - accuracy: 0.7848\n",
      "Epoch 33: loss improved from 0.87070 to 0.82111, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 0.8211 - accuracy: 0.7848\n",
      "Epoch 34/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.7747 - accuracy: 0.7985\n",
      "Epoch 34: loss improved from 0.82111 to 0.77474, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 58s 85ms/step - loss: 0.7747 - accuracy: 0.7985\n",
      "Epoch 35/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.7397 - accuracy: 0.8055\n",
      "Epoch 35: loss improved from 0.77474 to 0.73969, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 0.7397 - accuracy: 0.8055\n",
      "Epoch 36/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.6967 - accuracy: 0.8150\n",
      "Epoch 36: loss improved from 0.73969 to 0.69670, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.6967 - accuracy: 0.8150\n",
      "Epoch 37/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.6620 - accuracy: 0.8228\n",
      "Epoch 37: loss improved from 0.69670 to 0.66202, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 0.6620 - accuracy: 0.8228\n",
      "Epoch 38/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.6156 - accuracy: 0.8341\n",
      "Epoch 38: loss improved from 0.66202 to 0.61557, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 0.6156 - accuracy: 0.8341\n",
      "Epoch 39/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.5888 - accuracy: 0.8438\n",
      "Epoch 39: loss improved from 0.61557 to 0.58880, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.5888 - accuracy: 0.8438\n",
      "Epoch 40/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.5674 - accuracy: 0.8486\n",
      "Epoch 40: loss improved from 0.58880 to 0.56741, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 85ms/step - loss: 0.5674 - accuracy: 0.8486\n",
      "Epoch 41/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.5392 - accuracy: 0.8548\n",
      "Epoch 41: loss improved from 0.56741 to 0.53924, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 85ms/step - loss: 0.5392 - accuracy: 0.8548\n",
      "Epoch 42/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.8610\n",
      "Epoch 42: loss improved from 0.53924 to 0.51124, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.5112 - accuracy: 0.8610\n",
      "Epoch 43/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.4914 - accuracy: 0.8664\n",
      "Epoch 43: loss improved from 0.51124 to 0.49142, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 82ms/step - loss: 0.4914 - accuracy: 0.8664\n",
      "Epoch 44/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.4671 - accuracy: 0.8727\n",
      "Epoch 44: loss improved from 0.49142 to 0.46708, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.4671 - accuracy: 0.8727\n",
      "Epoch 45/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.4420 - accuracy: 0.8791\n",
      "Epoch 45: loss improved from 0.46708 to 0.44204, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 0.4420 - accuracy: 0.8791\n",
      "Epoch 46/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.8802\n",
      "Epoch 46: loss improved from 0.44204 to 0.43559, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 85ms/step - loss: 0.4356 - accuracy: 0.8802\n",
      "Epoch 47/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.4075 - accuracy: 0.8899\n",
      "Epoch 47: loss improved from 0.43559 to 0.40753, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.4075 - accuracy: 0.8899\n",
      "Epoch 48/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.3950 - accuracy: 0.8922\n",
      "Epoch 48: loss improved from 0.40753 to 0.39497, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 82ms/step - loss: 0.3950 - accuracy: 0.8922\n",
      "Epoch 49/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.3776 - accuracy: 0.8967\n",
      "Epoch 49: loss improved from 0.39497 to 0.37758, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.3776 - accuracy: 0.8967\n",
      "Epoch 50/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.3638 - accuracy: 0.8998\n",
      "Epoch 50: loss improved from 0.37758 to 0.36376, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 82ms/step - loss: 0.3638 - accuracy: 0.8998\n",
      "Epoch 51/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.3574 - accuracy: 0.9023\n",
      "Epoch 51: loss improved from 0.36376 to 0.35736, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 82ms/step - loss: 0.3574 - accuracy: 0.9023\n",
      "Epoch 52/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.3466 - accuracy: 0.9049\n",
      "Epoch 52: loss improved from 0.35736 to 0.34661, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.3466 - accuracy: 0.9049\n",
      "Epoch 53/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.3285 - accuracy: 0.9087\n",
      "Epoch 53: loss improved from 0.34661 to 0.32846, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 82ms/step - loss: 0.3285 - accuracy: 0.9087\n",
      "Epoch 54/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.3283 - accuracy: 0.9084\n",
      "Epoch 54: loss improved from 0.32846 to 0.32835, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 82ms/step - loss: 0.3283 - accuracy: 0.9084\n",
      "Epoch 55/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.3194 - accuracy: 0.9100\n",
      "Epoch 55: loss improved from 0.32835 to 0.31943, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 82ms/step - loss: 0.3194 - accuracy: 0.9100\n",
      "Epoch 56/100\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.9146\n",
      "Epoch 56: loss improved from 0.31943 to 0.30807, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 54s 80ms/step - loss: 0.3081 - accuracy: 0.9146\n",
      "Epoch 57/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2949 - accuracy: 0.9184\n",
      "Epoch 57: loss improved from 0.30807 to 0.29486, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 54s 79ms/step - loss: 0.2949 - accuracy: 0.9184\n",
      "Epoch 58/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2846 - accuracy: 0.9207\n",
      "Epoch 58: loss improved from 0.29486 to 0.28461, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 0.2846 - accuracy: 0.9207\n",
      "Epoch 59/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.9198\n",
      "Epoch 59: loss did not improve from 0.28461\n",
      "675/675 [==============================] - 54s 80ms/step - loss: 0.2855 - accuracy: 0.9198\n",
      "Epoch 60/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.9243\n",
      "Epoch 60: loss improved from 0.28461 to 0.27088, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 53s 79ms/step - loss: 0.2709 - accuracy: 0.9243\n",
      "Epoch 61/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9213\n",
      "Epoch 61: loss did not improve from 0.27088\n",
      "675/675 [==============================] - 54s 80ms/step - loss: 0.2763 - accuracy: 0.9213\n",
      "Epoch 62/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2619 - accuracy: 0.9255\n",
      "Epoch 62: loss improved from 0.27088 to 0.26194, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 0.2619 - accuracy: 0.9255\n",
      "Epoch 63/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.9315\n",
      "Epoch 63: loss improved from 0.26194 to 0.24442, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 82ms/step - loss: 0.2444 - accuracy: 0.9315\n",
      "Epoch 64/100\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.2477 - accuracy: 0.9301\n",
      "Epoch 64: loss did not improve from 0.24442\n",
      "675/675 [==============================] - 54s 80ms/step - loss: 0.2477 - accuracy: 0.9301\n",
      "Epoch 65/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.9280\n",
      "Epoch 65: loss did not improve from 0.24442\n",
      "675/675 [==============================] - 55s 82ms/step - loss: 0.2497 - accuracy: 0.9280\n",
      "Epoch 66/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9321\n",
      "Epoch 66: loss improved from 0.24442 to 0.23836, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.2384 - accuracy: 0.9321\n",
      "Epoch 67/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2369 - accuracy: 0.9321\n",
      "Epoch 67: loss improved from 0.23836 to 0.23689, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 82ms/step - loss: 0.2369 - accuracy: 0.9321\n",
      "Epoch 68/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2269 - accuracy: 0.9356\n",
      "Epoch 68: loss improved from 0.23689 to 0.22694, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.2269 - accuracy: 0.9356\n",
      "Epoch 69/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2276 - accuracy: 0.9351\n",
      "Epoch 69: loss did not improve from 0.22694\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.2276 - accuracy: 0.9351\n",
      "Epoch 70/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2317 - accuracy: 0.9327\n",
      "Epoch 70: loss did not improve from 0.22694\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.2317 - accuracy: 0.9327\n",
      "Epoch 71/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2276 - accuracy: 0.9341\n",
      "Epoch 71: loss did not improve from 0.22694\n",
      "675/675 [==============================] - 57s 85ms/step - loss: 0.2276 - accuracy: 0.9341\n",
      "Epoch 72/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.9386\n",
      "Epoch 72: loss improved from 0.22694 to 0.21758, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 58s 85ms/step - loss: 0.2176 - accuracy: 0.9386\n",
      "Epoch 73/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2108 - accuracy: 0.9394\n",
      "Epoch 73: loss improved from 0.21758 to 0.21076, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 82ms/step - loss: 0.2108 - accuracy: 0.9394\n",
      "Epoch 74/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2060 - accuracy: 0.9411\n",
      "Epoch 74: loss improved from 0.21076 to 0.20598, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.2060 - accuracy: 0.9411\n",
      "Epoch 75/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2144 - accuracy: 0.9384\n",
      "Epoch 75: loss did not improve from 0.20598\n",
      "675/675 [==============================] - 55s 82ms/step - loss: 0.2144 - accuracy: 0.9384\n",
      "Epoch 76/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 0.9379\n",
      "Epoch 76: loss did not improve from 0.20598\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 0.2148 - accuracy: 0.9379\n",
      "Epoch 77/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9421\n",
      "Epoch 77: loss improved from 0.20598 to 0.20308, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 0.2031 - accuracy: 0.9421\n",
      "Epoch 78/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2020 - accuracy: 0.9420\n",
      "Epoch 78: loss improved from 0.20308 to 0.20201, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 82ms/step - loss: 0.2020 - accuracy: 0.9420\n",
      "Epoch 79/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.2007 - accuracy: 0.9415\n",
      "Epoch 79: loss improved from 0.20201 to 0.20071, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 0.2007 - accuracy: 0.9415\n",
      "Epoch 80/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1971 - accuracy: 0.9433\n",
      "Epoch 80: loss improved from 0.20071 to 0.19712, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 54s 80ms/step - loss: 0.1971 - accuracy: 0.9433\n",
      "Epoch 81/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.9427\n",
      "Epoch 81: loss improved from 0.19712 to 0.19534, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 54s 81ms/step - loss: 0.1953 - accuracy: 0.9427\n",
      "Epoch 82/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9447\n",
      "Epoch 82: loss improved from 0.19534 to 0.19239, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 82ms/step - loss: 0.1924 - accuracy: 0.9447\n",
      "Epoch 83/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9435\n",
      "Epoch 83: loss improved from 0.19239 to 0.18960, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 0.1896 - accuracy: 0.9435\n",
      "Epoch 84/100\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.1862 - accuracy: 0.9451\n",
      "Epoch 84: loss improved from 0.18960 to 0.18620, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 82ms/step - loss: 0.1862 - accuracy: 0.9450\n",
      "Epoch 85/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.9448\n",
      "Epoch 85: loss did not improve from 0.18620\n",
      "675/675 [==============================] - 55s 82ms/step - loss: 0.1912 - accuracy: 0.9448\n",
      "Epoch 86/100\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.1859 - accuracy: 0.9460\n",
      "Epoch 86: loss improved from 0.18620 to 0.18594, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 0.1859 - accuracy: 0.9460\n",
      "Epoch 87/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1725 - accuracy: 0.9500\n",
      "Epoch 87: loss improved from 0.18594 to 0.17252, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.1725 - accuracy: 0.9500\n",
      "Epoch 88/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.9513\n",
      "Epoch 88: loss did not improve from 0.17252\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 0.1726 - accuracy: 0.9513\n",
      "Epoch 89/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1779 - accuracy: 0.9484\n",
      "Epoch 89: loss did not improve from 0.17252\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.1779 - accuracy: 0.9484\n",
      "Epoch 90/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1781 - accuracy: 0.9477\n",
      "Epoch 90: loss did not improve from 0.17252\n",
      "675/675 [==============================] - 57s 84ms/step - loss: 0.1781 - accuracy: 0.9477\n",
      "Epoch 91/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1736 - accuracy: 0.9487\n",
      "Epoch 91: loss did not improve from 0.17252\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 0.1736 - accuracy: 0.9487\n",
      "Epoch 92/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9483\n",
      "Epoch 92: loss did not improve from 0.17252\n",
      "675/675 [==============================] - 54s 80ms/step - loss: 0.1739 - accuracy: 0.9483\n",
      "Epoch 93/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1767 - accuracy: 0.9473\n",
      "Epoch 93: loss did not improve from 0.17252\n",
      "675/675 [==============================] - 55s 82ms/step - loss: 0.1767 - accuracy: 0.9473\n",
      "Epoch 94/100\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.1703 - accuracy: 0.9507\n",
      "Epoch 94: loss improved from 0.17252 to 0.17036, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 0.1704 - accuracy: 0.9507\n",
      "Epoch 95/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9508\n",
      "Epoch 95: loss improved from 0.17036 to 0.16816, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 56s 82ms/step - loss: 0.1682 - accuracy: 0.9508\n",
      "Epoch 96/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1710 - accuracy: 0.9495\n",
      "Epoch 96: loss did not improve from 0.16816\n",
      "675/675 [==============================] - 56s 83ms/step - loss: 0.1710 - accuracy: 0.9495\n",
      "Epoch 97/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9503\n",
      "Epoch 97: loss did not improve from 0.16816\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 0.1689 - accuracy: 0.9503\n",
      "Epoch 98/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1647 - accuracy: 0.9508\n",
      "Epoch 98: loss improved from 0.16816 to 0.16466, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 0.1647 - accuracy: 0.9508\n",
      "Epoch 99/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1564 - accuracy: 0.9545\n",
      "Epoch 99: loss improved from 0.16466 to 0.15642, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 55s 81ms/step - loss: 0.1564 - accuracy: 0.9545\n",
      "Epoch 100/100\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9545\n",
      "Epoch 100: loss improved from 0.15642 to 0.15543, saving model to weights-best1.hdf5\n",
      "675/675 [==============================] - 57s 85ms/step - loss: 0.1554 - accuracy: 0.9545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1688a3c4c40>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"weights-best1.hdf5\", monitor=\"loss\", save_best_only=True, verbose=1)\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "# Train the Model\n",
    "model.fit(X, y, epochs=100, batch_size=64, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a489371-24fe-44a5-821f-0588970bbd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "coping problemsolving and interpersonal skills and learning to manage emotions protective and supportive environments in the family at school and in the wider community are important multiple factors affect mental health the more risk factors adolescents are exposed to the greater the potential impact on their mental health factors that can contribute to stress during adolescence include exposure to adversity pressure to conform with peers and exploration of identity media influence and gender norms can exacerbate the disparity between an adolescents lived reality and their perceptions or aspirations for the future other important determinants include the quality of their home life and relationships with peers violence especially sexual violence and bullying harsh parenting and severe and socioeconomic problems are recognized risks to mental health some adolescents are at greater risk of mental health conditions due to their living conditions stigma discrimination or exclusion or lack of access to quality support and services these include adolescents living in humanitarian and fragile settings adolescents with chronic illness autism spectrum disorder an intellectual disability or other neurological condition pregnant adolescents adolescent parents or those in early or forced marriages orphans and adolescents from minority ethnic or sexual backgrounds or other discriminated groups emotional disorders emotional disorders are common among adolescents anxiety disorders which may involve panic or excessive worry are the most prevalent in this age group and are more common among older than among younger adolescents it is estimated that 44 of 1014yearolds and 55 of 1519yearolds experience an anxiety disorder 1 depression is estimated to occur among 14 of adolescents aged 1014 years and 35 of 1519yearolds 1 depression and anxiety share some of the same symptoms including rapid and unexpected changes in mood anxiety and depressive disorders can profoundly affect school attendance and schoolwork social withdrawal can exacerbate isolation and loneliness depression can lead to suicide behavioural disorders behavioural disorders are more common among younger adolescents than older adolescents attention deficit hyperactivity disorder adhd characterized by difficulty paying attention andor excessive activity and acting without regard to consequences occurs among 29 of 1014yearolds and 22 of 1519yearolds 1 conduct disorder involving symptoms of destructive or challenging behaviour occurs among 35 of 1014yearolds and 19 of 1519yearolds 1 behavioural disorders can affect adolescents education and increases the risk of criminal behaviour eating disorders eating disorders such as anorexia nervosa and bulimia nervosa commonly emerge during adolescence and young adulthood eating disorders involve abnormal eating behaviour and preoccupation with food accompanied in most instances by concerns about body weight and shape girls are more commonly affected than boys eating disorders can affect physical health and often coexist with depression anxiety and substance use disorders they occur in an estimated 01 of 1014yearolds and and 04 of 1519yearolds 1 they are associated with suicide anorexia nervosa can lead to premature death often due to medical complications or suicide and has higher mortality than any other mental disorder psychosis conditions that include symptoms of psychosis most commonly emerge in late adolescence or early adulthood symptoms can include hallucinations or delusions these experiences can impair an adolescents ability to participate in daily life and education and often lead to stigma or human rights violations schizophrenia occurs in 01 of 1519yearolds 1 suicide and selfharm suicide is the third leading cause of death in older adolescents and young adults 1529 years 2 risk factors for suicide are multifaceted and include harmful use of alcohol abuse in childhood stigma against helpseeking barriers to accessing care and access to means of suicide digital media like any other media can play a significant role in either enhancing or weakening suicide prevention efforts risktaking behaviours many risktaking behaviours for health such as substance use or sexual risktaking start during adolescence risktaking behaviours can be an unhelpful strategy to cope with emotional difficulties and can severely impact an adolescents mental and physical wellbeing young people are especially vulnerable to developing harmful substance use patterns that can persist across the lifespan in 2019 the prevalence of alcohol use among 1519yearolds was high worldwide 22 with very few gender differences and showing an increase in consumption in some regions 3 the use of tobacco and cannabis are additional concerns many adult smokers had their first cigarette prior to the age of 18 years in 2022 the prevalence of cannabis use among adolescents was higher than that of adults globally 55 per cent compared with 44 per cent respectively 4 perpetration of violence is a risktaking behaviour that can increase the likelihood of low educational attainment injury involvement with crime or death interpersonal violence was ranked among the leading causes of death of older adolescents in 2021 1 promotion and prevention mental health promotion and prevention interventions aim to strengthen an individuals capacity to regulate emotions enhance alternatives to risktaking behaviours build resilience for managing difficult situations and adversity and promote supportive social environments and social networks these programmes require a multilevel approach with varied delivery platforms for example digital media health or social care settings schools or the community and varied strategies to reach adolescents particularly the most vulnerable early detection and treatment it is crucial to address the needs of adolescents with mental health conditions avoiding institutionalization and overmedicalization prioritizing nonpharmacological approaches and respecting the rights of children in line with the united nations convention on the rights of the child and other human rights instruments are key for adolescents mental health who response who works on strategies programmes and tools to assist governments in responding to the health needs of adolescents for example the helping adolescents thrive hat initiative is a joint whounicef effort to strengthen policies and programmes for the mental health of adolescents more specifically the efforts made through the initiative are to promote mental health and prevent mental health conditions they are also intended to help prevent selfharm and other risk behaviours such as harmful use of alcohol and drugs that have a negative impact on the mental and physical health of young people who has also developed\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"weights-best1.hdf5\")\n",
    "\n",
    "# Generate Text\n",
    "seed_idx = np.random.randint(0, len(X) - 1)\n",
    "seed_sequence = X[seed_idx]\n",
    "\n",
    "output = []\n",
    "p=0\n",
    "for _ in range(1000):  # Generate 1000 characters\n",
    "    if i==\".\":\n",
    "        p+=1\n",
    "        if p==2:\n",
    "            break \n",
    "    pred_input = np.reshape(seed_sequence, (1, len(seed_sequence)))\n",
    "    pred_probs = model.predict(pred_input, verbose=0)\n",
    "    next_idx = np.argmax(pred_probs)\n",
    "    output.append(tokenizer.index_word[next_idx])\n",
    "\n",
    "    # Update seed sequence\n",
    "    seed_sequence = np.append(seed_sequence[1:], next_idx)\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(\" \".join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0698fb40-c3c9-4aab-b328-760df270b392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some information related to your query:\n",
      "I don't have specific information about that. Let's explore general advice.\n",
      "\n",
      "Chatbot: and worry in the same risk of mental health conditions are not not have a mental health condition is not not have a mental health condition is not not have a mental illness is not not not have a person has a person has a person has a person has a person has a person has a person has a person has a person has a person has a person has a person has a person has a person has a person has a person has a person has a person has a person has a person has a person\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the trained model\n",
    "model.load_weights(\"weights-best.hdf5\")\n",
    "\n",
    "# Define the knowledge base\n",
    "with open('mental_h.txt', 'r', encoding='utf-8') as file:\n",
    "    knowledge_base = file.read()\n",
    "\n",
    "# Extract knowledge base sections\n",
    "def find_relevant_info(user_input, knowledge_text):\n",
    "    vectorizer = TfidfVectorizer().fit_transform([knowledge_text, user_input])\n",
    "    similarity = vectorizer.toarray().dot(vectorizer.toarray().T)[0, 1]\n",
    "    if similarity > 0.1:\n",
    "        # Extract sentences with relevance\n",
    "        return '\\n'.join([sentence for sentence in knowledge_text.splitlines() if user_input.lower() in sentence.lower()])\n",
    "    return \"I don't have specific information about that. Let's explore general advice.\"\n",
    "\n",
    "# Generate a creative response\n",
    "def generate_response(user_input, tokenizer, model, max_sequence_length, output_length=100):\n",
    "    input_sequence = tokenizer.texts_to_sequences([user_input])\n",
    "    input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "    output = []\n",
    "    for _ in range(output_length):\n",
    "        pred_probs = model.predict(input_sequence, verbose=0)\n",
    "        next_idx = np.argmax(pred_probs)\n",
    "        output.append(tokenizer.index_word.get(next_idx, \"\"))\n",
    "\n",
    "        input_sequence = np.append(input_sequence[0][1:], next_idx).reshape(1, max_sequence_length)\n",
    "\n",
    "    return \" \".join(output)\n",
    "\n",
    "# Example usage\n",
    "user_input = input(\"You: \")\n",
    "relevant_info = find_relevant_info(user_input, knowledge_base)\n",
    "creative_response = generate_response(user_input, tokenizer, model, max_sequence_length=100, output_length=100)\n",
    "\n",
    "response = f\"Here is some information related to your query:\\n{relevant_info}\\n\\nChatbot: {creative_response}\"\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdc7998b-e40f-4e89-a4b6-b13c19bce245",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     pred_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(seed_sequence, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(seed_sequence)))\n\u001b[1;32m---> 25\u001b[0m     pred_probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     next_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(pred_probs)\n\u001b[0;32m     27\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(tokenizer\u001b[38;5;241m.\u001b[39mindex_word[next_idx])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\keras\\engine\\training.py:2220\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2211\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   2212\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2213\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2214\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2217\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   2218\u001b[0m         )\n\u001b[1;32m-> 2220\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2224\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2228\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2230\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\keras\\engine\\data_adapter.py:1582\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\keras\\engine\\data_adapter.py:1262\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[0;32m   1261\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1277\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\keras\\engine\\data_adapter.py:347\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    344\u001b[0m         flat_dataset \u001b[38;5;241m=\u001b[39m flat_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(epochs)\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m--> 347\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_batch_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2245\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[1;34m(self, map_func, name)\u001b[0m\n\u001b[0;32m   2212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_map\u001b[39m(\u001b[38;5;28mself\u001b[39m, map_func, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2213\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Maps `map_func` across this dataset and flattens the result.\u001b[39;00m\n\u001b[0;32m   2214\u001b[0m \n\u001b[0;32m   2215\u001b[0m \u001b[38;5;124;03m  The type signature is:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2245\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFlatMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5484\u001b[0m, in \u001b[0;36mFlatMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[0;32m   5482\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m   5483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[1;32m-> 5484\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure, DatasetSpec):\n\u001b[0;32m   5487\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   5488\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `map_func` argument must return a `Dataset` object. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5489\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_get_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    265\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    269\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2610\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2602\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m   2603\u001b[0m \n\u001b[0;32m   2604\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2608\u001b[0m \u001b[38;5;124;03m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[0;32m   2609\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2610\u001b[0m   graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_concrete_function_garbage_collected(\n\u001b[0;32m   2611\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2612\u001b[0m   graph_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2613\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2576\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2574\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m-> 2576\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2577\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m   2578\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[0;32m   2579\u001b[0m       graph_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2760\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2758\u001b[0m   \u001b[38;5;66;03m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[0;32m   2759\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m placeholder_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m-> 2760\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2762\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m graph_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39m_capture_func_lib  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2763\u001b[0m \u001b[38;5;66;03m# Maintain the list of all captures\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2670\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2665\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2666\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   2667\u001b[0m ]\n\u001b[0;32m   2668\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   2669\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 2670\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2671\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2672\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2673\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2675\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2678\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   2680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   2681\u001b[0m     spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   2682\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   2684\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   2686\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2687\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1247\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1245\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1247\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m   1252\u001b[0m     convert, func_outputs, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:248\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;129m@eager_function\u001b[39m\u001b[38;5;241m.\u001b[39mdefun_with_attributes(\n\u001b[0;32m    243\u001b[0m     input_signature\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_specs(\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_structure),\n\u001b[0;32m    245\u001b[0m     autograph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    246\u001b[0m     attributes\u001b[38;5;241m=\u001b[39mdefun_kwargs)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:177\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[0;32m    176\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[1;32m--> 177\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n\u001b[0;32m    179\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ret)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    455\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 458\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\keras\\engine\\data_adapter.py:340\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__.<locals>.slice_batch_indices\u001b[1;34m(indices)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partial_batch_size:\n\u001b[0;32m    335\u001b[0m     index_remainder \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensors(\n\u001b[0;32m    336\u001b[0m         tf\u001b[38;5;241m.\u001b[39mslice(\n\u001b[0;32m    337\u001b[0m             indices, [num_in_full_batch], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partial_batch_size]\n\u001b[0;32m    338\u001b[0m         )\n\u001b[0;32m    339\u001b[0m     )\n\u001b[1;32m--> 340\u001b[0m     flat_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mflat_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_remainder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;66;03m# 1024 is a magic constant that has not been properly evaluated\u001b[39;00m\n\u001b[0;32m    344\u001b[0m     flat_dataset \u001b[38;5;241m=\u001b[39m flat_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(epochs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1294\u001b[0m, in \u001b[0;36mDatasetV2.concatenate\u001b[1;34m(self, dataset, name)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcatenate\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a `Dataset` by concatenating the given dataset with this dataset.\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m \n\u001b[0;32m   1269\u001b[0m \u001b[38;5;124;03m  >>> a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1294\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mConcatenateDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4894\u001b[0m, in \u001b[0;36mConcatenateDataset.__init__\u001b[1;34m(self, input_dataset, dataset_to_concatenate, name)\u001b[0m\n\u001b[0;32m   4892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m   4893\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 4894\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mconcatenate_dataset(\n\u001b[0;32m   4895\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor, dataset_to_concatenate\u001b[38;5;241m.\u001b[39m_variant_tensor,\n\u001b[0;32m   4896\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n\u001b[0;32m   4897\u001b[0m \u001b[38;5;66;03m# pylint: enable=protected-access\u001b[39;00m\n\u001b[0;32m   4898\u001b[0m \u001b[38;5;28msuper\u001b[39m(ConcatenateDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(variant_tensor)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:1102\u001b[0m, in \u001b[0;36mconcatenate_dataset\u001b[1;34m(input_dataset, another_dataset, output_types, output_shapes, metadata, name)\u001b[0m\n\u001b[0;32m   1100\u001b[0m   metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1101\u001b[0m metadata \u001b[38;5;241m=\u001b[39m _execute\u001b[38;5;241m.\u001b[39mmake_str(metadata, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1102\u001b[0m _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_op_def_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConcatenateDataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m                            \u001b[49m\u001b[43manother_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manother_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m                            \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m                            \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1108\u001b[0m _result \u001b[38;5;241m=\u001b[39m _outputs[:]\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:797\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    792\u001b[0m must_colocate_inputs \u001b[38;5;241m=\u001b[39m [val \u001b[38;5;28;01mfor\u001b[39;00m arg, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(op_def\u001b[38;5;241m.\u001b[39minput_arg, inputs)\n\u001b[0;32m    793\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mis_ref]\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[0;32m    795\u001b[0m   \u001b[38;5;66;03m# Add Op to graph\u001b[39;00m\n\u001b[0;32m    796\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 797\u001b[0m   op \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattr_protos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# for more details.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m outputs \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39moutputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:735\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    733\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    734\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[1;32m--> 735\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFuncGraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3800\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3797\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 3800\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3801\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3802\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3805\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3807\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3809\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2108\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   2105\u001b[0m     control_input_ops\u001b[38;5;241m.\u001b[39mappend(control_op)\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 2108\u001b[0m c_op \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_input_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_from_c_op(c_op\u001b[38;5;241m=\u001b[39mc_op, g\u001b[38;5;241m=\u001b[39mg)\n\u001b[0;32m   2111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_op \u001b[38;5;241m=\u001b[39m original_op\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1966\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1962\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(name),\n\u001b[0;32m   1963\u001b[0m                                          serialized)\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1966\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_FinishOperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_desc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1967\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1968\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.load_weights(\"weights-best.hdf5\")\n",
    "\n",
    "# Generate Text\n",
    "seed_idx = np.random.randint(0, len(X) - 1)\n",
    "seed_sequence = X[seed_idx]\n",
    "\n",
    "output = []\n",
    "temperature = 1.0  # Lower values make text more deterministic; higher values make it more diverse.\n",
    "\n",
    "for i in range(2000):  # Generate 1000 characters\n",
    "    if i<1500:\n",
    "        pred_input = np.reshape(seed_sequence, (1, len(seed_sequence)))\n",
    "        pred_probs = model.predict(pred_input, verbose=0)\n",
    "        next_idx = np.argmax(pred_probs)\n",
    "        output.append(tokenizer.index_word[next_idx])\n",
    "\n",
    "    # Update seed sequence\n",
    "        seed_sequence = np.append(seed_sequence[1:], next_idx)\n",
    "        \n",
    "    else:\n",
    "        if seed_idx == \".\":\n",
    "            break\n",
    "        else:\n",
    "            pred_input = np.reshape(seed_sequence, (1, len(seed_sequence)))\n",
    "            pred_probs = model.predict(pred_input, verbose=0)\n",
    "            next_idx = np.argmax(pred_probs)\n",
    "            output.append(tokenizer.index_word[next_idx])\n",
    "\n",
    "            # Update seed sequence\n",
    "            seed_sequence = np.append(seed_sequence[1:], next_idx)\n",
    "            \n",
    "        \n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(\"\".join(output)+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d30ed-4b77-46c1-890f-550c1c372995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
