{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f1fb7b-02ae-420b-8d7d-f0b5dd3299bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Load and Preprocess Data\n",
    "filename = \"mental_H.txt\"\n",
    "with open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "    raw_text = file.read().lower()\n",
    "import re\n",
    "\n",
    "# Remove unnecessary characters and normalize text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = text.strip().lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "raw_text = clean_text(raw_text)\n",
    "\n",
    "# Tokenization\n",
    "##\n",
    "tokenizer = Tokenizer(num_words=5000)  # Limit vocab size to 5000\n",
    "tokenizer.fit_on_texts([raw_text])\n",
    "##\n",
    "sequences = tokenizer.texts_to_sequences([raw_text])[0]\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "##\n",
    "seq_length = 100\n",
    "\n",
    "# Create Input-Output Pairs\n",
    "X = []\n",
    "y = []\n",
    "for i in range(0, len(sequences) - seq_length):\n",
    "    X.append(sequences[i:i + seq_length])\n",
    "    y.append(sequences[i + seq_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "# Reshape for LSTM Input\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1]))\n",
    "\n",
    "# Model Definition\n",
    "##\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 256, input_length=seq_length),\n",
    "    Bidirectional(LSTM(256, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    Bidirectional(LSTM(256)),\n",
    "    Dropout(0.3),\n",
    "    Dense(vocab_size, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a27254-b1a5-4f8d-9028-b566fafcc222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 512)          2589184   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100, 1024)        4198400   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 1024)         0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 100, 1024)        4096      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 1024)             6295552   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5057)              2594241   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,206,273\n",
      "Trainable params: 16,204,225\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e75f473-e0a1-4d62-aae4-b8f3779a7a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nFailed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 1024, 512, 1, 100, 64, 512] \n\t [[{{node CudnnRNN}}]]\n\t [[sequential/bidirectional_1/backward_lstm_1/PartitionedCall]] [Op:__inference_train_function_11039]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [checkpoint]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the Model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_1\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInternalError\u001b[0m: Graph execution error:\n\nFailed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 1024, 512, 1, 100, 64, 512] \n\t [[{{node CudnnRNN}}]]\n\t [[sequential/bidirectional_1/backward_lstm_1/PartitionedCall]] [Op:__inference_train_function_11039]"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"weights-best1.hdf5\", monitor=\"loss\", save_best_only=True, verbose=1)\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "# Train the Model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a489371-24fe-44a5-821f-0588970bbd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"weights-best.hdf5\")\n",
    "\n",
    "# Generate Text\n",
    "seed_idx = np.random.randint(0, len(X) - 1)\n",
    "seed_sequence = X[seed_idx]\n",
    "\n",
    "output = []\n",
    "for _ in range(1000):  # Generate 1000 characters\n",
    "    pred_input = np.reshape(seed_sequence, (1, len(seed_sequence)))\n",
    "    pred_probs = model.predict(pred_input, verbose=0)\n",
    "    next_idx = np.argmax(pred_probs)\n",
    "    output.append(tokenizer.index_word[next_idx])\n",
    "\n",
    "    # Update seed sequence\n",
    "    seed_sequence = np.append(seed_sequence[1:], next_idx)\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(\"\".join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0698fb40-c3c9-4aab-b328-760df270b392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is the stress\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some information related to your query:\n",
      "\n",
      "\n",
      "Chatbot: es of mental health conditions in the process to a person with a mental health conditions and contro\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the trained model\n",
    "model.load_weights(\"weights-best.hdf5\")\n",
    "\n",
    "# Define the knowledge base\n",
    "with open('mental_h.txt', 'r', encoding='utf-8') as file:\n",
    "    knowledge_base = file.read()\n",
    "\n",
    "# Extract knowledge base sections\n",
    "def find_relevant_info(user_input, knowledge_text):\n",
    "    vectorizer = TfidfVectorizer().fit_transform([knowledge_text, user_input])\n",
    "    similarity = vectorizer.toarray().dot(vectorizer.toarray().T)[0, 1]\n",
    "    if similarity > 0.1:\n",
    "        # Extract sentences with relevance\n",
    "        return '\\n'.join([sentence for sentence in knowledge_text.splitlines() if user_input.lower() in sentence.lower()])\n",
    "    return \"I don't have specific information about that. Let's explore general advice.\"\n",
    "\n",
    "# Generate a creative response\n",
    "def generate_response(user_input, tokenizer, model, max_sequence_length, output_length=100):\n",
    "    input_sequence = tokenizer.texts_to_sequences([user_input])\n",
    "    input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "    output = []\n",
    "    for _ in range(output_length):\n",
    "        pred_probs = model.predict(input_sequence, verbose=0)\n",
    "        next_idx = np.argmax(pred_probs)\n",
    "        output.append(tokenizer.index_word.get(next_idx, \"\"))\n",
    "\n",
    "        input_sequence = np.append(input_sequence[0][1:], next_idx).reshape(1, max_sequence_length)\n",
    "\n",
    "    return \"\".join(output)\n",
    "\n",
    "# Example usage\n",
    "user_input = input(\"You: \")\n",
    "relevant_info = find_relevant_info(user_input, knowledge_base)\n",
    "creative_response = generate_response(user_input, tokenizer, model, max_sequence_length=100, output_length=100)\n",
    "\n",
    "response = f\"Here is some information related to your query:\\n{relevant_info}\\n\\nChatbot: {creative_response}\"\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc7998b-e40f-4e89-a4b6-b13c19bce245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "e mental health professionals and support to seek treatment options and can help a person’s mental health is a basic and include problems and support to more than the stressors are also present and prevent mental health at work is a loss of person continuity of suicide and substance use disorders in adults with mental health conditions and psychosocial disasters and suicide and well-being.\n",
      "\n",
      "the development of mental health conditions in the brain that are several family, and individuals may feel an important to recommends person to mental illnesses and the professional will recommend that a person’s behavior and support workers with bipolar disorder (adhd) is a common mental health conditions are more likely to experience personal activity and antidepressant medications and an individual with a mental health professionals and support to a depressive episode and depression in the brain that they are more likely to develop better services and support to provide support to the internal for physical health care and treatment of mental health conditions and psychosocial support and support from the stressors are more likely to develop a mental health condition that become the brain common mental health at work and environmental services and complex medications that may be improved to mental health conditions and the mental health condition that are more likely to experience a person’s mental health at work is a basic and severe depression, anxiety, and suicide is a service part of the prevalence of mental health conditions. the mental health condition that are also present and that the presynaptic neuron that they may also be provided to treat mental health conditions and psychosocial risk to suicide and self-harm has been control of the mediterranean diet that are provided to seek help from the brain that promote the mental health condition to the person’s mental health is a setting to support the prevalence of mental health conditions can be a person’s mental health co.\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"weights-best.hdf5\")\n",
    "\n",
    "# Generate Text\n",
    "seed_idx = np.random.randint(0, len(X) - 1)\n",
    "seed_sequence = X[seed_idx]\n",
    "\n",
    "output = []\n",
    "temperature = 1.0  # Lower values make text more deterministic; higher values make it more diverse.\n",
    "\n",
    "for i in range(2000):  # Generate 1000 characters\n",
    "    if i<1500:\n",
    "        pred_input = np.reshape(seed_sequence, (1, len(seed_sequence)))\n",
    "        pred_probs = model.predict(pred_input, verbose=0)\n",
    "        next_idx = np.argmax(pred_probs)\n",
    "        output.append(tokenizer.index_word[next_idx])\n",
    "\n",
    "    # Update seed sequence\n",
    "        seed_sequence = np.append(seed_sequence[1:], next_idx)\n",
    "        \n",
    "    else:\n",
    "        if seed_idx == \".\":\n",
    "            break\n",
    "        else:\n",
    "            pred_input = np.reshape(seed_sequence, (1, len(seed_sequence)))\n",
    "            pred_probs = model.predict(pred_input, verbose=0)\n",
    "            next_idx = np.argmax(pred_probs)\n",
    "            output.append(tokenizer.index_word[next_idx])\n",
    "\n",
    "            # Update seed sequence\n",
    "            seed_sequence = np.append(seed_sequence[1:], next_idx)\n",
    "            \n",
    "        \n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(\"\".join(output)+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d30ed-4b77-46c1-890f-550c1c372995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
